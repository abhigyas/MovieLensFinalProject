{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "df = pd.read_csv(\"databases/ml-latest-small/ratings.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    \n",
    "    # The Movie Lens Dataset class. This class prepares the dataset for training and validation.\n",
    "    \n",
    "    def __init__(self, users, movies, ratings):\n",
    "        # Initializes the dataset object with user, movie, and rating data.\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the total number of samples in the dataset.\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "       \n",
    "        # Retrieves a sample from the dataset at the specified index.\n",
    "        \n",
    "        users = self.users[item]\n",
    "        movies = self.movies[item]\n",
    "        ratings = self.ratings[item]\n",
    "\n",
    "        return {\n",
    "            \"users\": torch.tensor(users, dtype=torch.long),\n",
    "            \"movies\": torch.tensor(movies, dtype=torch.long),\n",
    "            \"ratings\": torch.tensor(ratings, dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "class RecommendationSystemModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users,\n",
    "        num_movies,\n",
    "        embedding_size=256,\n",
    "        hidden_dim=256,\n",
    "        dropout_rate=0.2,\n",
    "    ):\n",
    "        super(RecommendationSystemModel, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layers\n",
    "        self.user_embedding = nn.Embedding(\n",
    "            num_embeddings=self.num_users, embedding_dim=self.embedding_size\n",
    "        )\n",
    "        self.movie_embedding = nn.Embedding(\n",
    "            num_embeddings=self.num_movies, embedding_dim=self.embedding_size\n",
    "        )\n",
    "\n",
    "        # Hidden layers\n",
    "        self.fc1 = nn.Linear(2 * self.embedding_size, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, users, movies):\n",
    "        # Embeddings\n",
    "        user_embedded = self.user_embedding(users)\n",
    "        movie_embedded = self.movie_embedding(movies)\n",
    "\n",
    "        # Concatenate user and movie embeddings\n",
    "        combined = torch.cat([user_embedded, movie_embedded], dim=1)\n",
    "\n",
    "        # Pass through hidden layers with ReLU activation and dropout\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 90752 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 90752/90752, Loss: 0.114584"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 90752/90752, Loss: 0.079081"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 90752/90752, Loss: 0.088100"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 90752/90752, Loss: 0.078816"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 90752/90752, Loss: 0.068416"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 90752/90752, Loss: 0.072012"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 90752/90752, Loss: 0.059650"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 90752/90752, Loss: 0.074330"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, Step 90752/90752, Loss: 0.064012"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 90752/90752, Loss: 0.075464"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE: 0.8852\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "class ImprovedRecommendationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users,\n",
    "        num_movies,\n",
    "        embedding_size=256,\n",
    "        hidden_dims=[512, 256, 128],\n",
    "        dropout_rate=0.2,\n",
    "    ):\n",
    "        super(ImprovedRecommendationModel, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Embedding layers\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_size)\n",
    "\n",
    "        # Batch normalization for embeddings\n",
    "        self.bn_user = nn.BatchNorm1d(embedding_size)\n",
    "        self.bn_movie = nn.BatchNorm1d(embedding_size)\n",
    "\n",
    "        # Build MLP layers with residual connections\n",
    "        layers = []\n",
    "        input_dim = 2 * embedding_size\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.final = nn.Linear(hidden_dims[-1], 1)\n",
    "        \n",
    "        # Residual connection\n",
    "        self.residual = nn.Linear(2 * embedding_size, 1)\n",
    "\n",
    "    def forward(self, users, movies):\n",
    "        # Get embeddings\n",
    "        user_embedded = self.user_embedding(users)\n",
    "        movie_embedded = self.movie_embedding(movies)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        user_embedded = self.bn_user(user_embedded)\n",
    "        movie_embedded = self.bn_movie(movie_embedded)\n",
    "\n",
    "        # Concatenate\n",
    "        combined = torch.cat([user_embedded, movie_embedded], dim=1)\n",
    "        \n",
    "        # Main network path\n",
    "        x = self.mlp(combined)\n",
    "        main_output = self.final(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        res_output = self.residual(combined)\n",
    "        \n",
    "        # Combine main and residual paths\n",
    "        output = main_output + res_output\n",
    "        \n",
    "        return output.squeeze()\n",
    "from sklearn import model_selection\n",
    "\n",
    "df_train, df_val = model_selection.train_test_split(\n",
    "    df, test_size=0.1, random_state=3, stratify=df.rating.values\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(df_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(df_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "#\n",
    "\n",
    "import sys\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming df_train and df_val are already defined\n",
    "le_user = LabelEncoder()\n",
    "le_movie = LabelEncoder()\n",
    "\n",
    "# Combine userId and movieId columns from both training and validation datasets\n",
    "combined_user_ids = pd.concat([df_train['userId'], df_val['userId']])\n",
    "combined_movie_ids = pd.concat([df_train['movieId'], df_val['movieId']])\n",
    "\n",
    "# Fit the LabelEncoder on the combined data\n",
    "le_user.fit(combined_user_ids)\n",
    "le_movie.fit(combined_movie_ids)\n",
    "\n",
    "# Transform the userId and movieId columns in both datasets\n",
    "df_train['userId'] = le_user.transform(df_train['userId'])\n",
    "df_train['movieId'] = le_movie.transform(df_train['movieId'])\n",
    "df_val['userId'] = le_user.transform(df_val['userId'])\n",
    "df_val['movieId'] = le_movie.transform(df_val['movieId'])\n",
    "\n",
    "train_dataset = MovieLensDataset(\n",
    "    users=df_train['userId'].values,\n",
    "    movies=df_train['movieId'].values,\n",
    "    ratings=df_train['rating'].values\n",
    ")\n",
    "\n",
    "val_dataset = MovieLensDataset(\n",
    "    users=df_val['userId'].values,\n",
    "    movies=df_val['movieId'].values,\n",
    "    ratings=df_val['rating'].values\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "recommendation_model = ImprovedRecommendationModel(\n",
    "    num_users=len(le_user.classes_), \n",
    "    num_movies=len(le_movie.classes_),\n",
    "    embedding_size=256,\n",
    "    hidden_dims=[512, 256, 128],\n",
    "    dropout_rate=0.2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(recommendation_model.parameters(), lr=1e-3)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# Function to log progress\n",
    "def log_progress(epoch, step, total_loss, log_progress_step, data_size, losses):\n",
    "    avg_loss = total_loss / log_progress_step\n",
    "    sys.stderr.write(f\"\\rEpoch {epoch}, Step {step}/{data_size}, Loss: {avg_loss:.6f}\")\n",
    "    sys.stderr.flush()\n",
    "\n",
    "total_loss = 0\n",
    "log_progress_step = 100\n",
    "losses = []\n",
    "train_dataset_size = len(train_dataset)\n",
    "print(f\"Training on {train_dataset_size} samples...\")\n",
    "\n",
    "recommendation_model.train()\n",
    "for e in range(EPOCHS):\n",
    "    step_count = 0  # Reset step count at the beginning of each epoch\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        output = recommendation_model(\n",
    "            train_data[\"users\"].to(device), train_data[\"movies\"].to(device)\n",
    "        )\n",
    "        # Reshape the model output to match the target's shape\n",
    "        output = output.squeeze()  # Removes the singleton dimension\n",
    "        ratings = (\n",
    "            train_data[\"ratings\"].to(torch.float32).to(device)\n",
    "        )  # Assuming ratings is already 1D\n",
    "\n",
    "        loss = loss_func(output, ratings)\n",
    "        total_loss += loss.sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Increment step count by the actual size of the batch\n",
    "        step_count += len(train_data[\"users\"])\n",
    "\n",
    "        # Check if it's time to log progress\n",
    "        if (\n",
    "            step_count % log_progress_step == 0 or i == len(train_loader) - 1\n",
    "        ):  # Log at the end of each epoch\n",
    "            log_progress(\n",
    "                e, step_count, total_loss, log_progress_step, train_dataset_size, losses\n",
    "            )\n",
    "            total_loss = 0\n",
    "    print()  # Move to the next line after each epoch\n",
    "\n",
    "# Evaluation\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "recommendation_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, valid_data in enumerate(val_loader):\n",
    "        output = recommendation_model(\n",
    "            valid_data[\"users\"].to(device), valid_data[\"movies\"].to(device)\n",
    "        )\n",
    "        ratings = valid_data[\"ratings\"].to(device)\n",
    "        y_pred.extend(output.cpu().numpy())\n",
    "        y_true.extend(ratings.cpu().numpy())\n",
    "\n",
    "# Calculate RMSE\n",
    "rms = root_mean_squared_error(y_true, y_pred)\n",
    "print(f\"RMSE: {rms:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "df_train, df_val = model_selection.train_test_split(\n",
    "    df, test_size=0.1, random_state=3, stratify=df.rating.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(df_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(df_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 90752 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 800/90752, Loss: 0.19140064"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 800/90752, Loss: 0.18684624"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 800/90752, Loss: 0.16705787"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 800/90752, Loss: 0.16671784"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 800/90752, Loss: 0.14875502"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 800/90752, Loss: 0.14216486"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 800/90752, Loss: 0.12799800"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, Step 800/90752, Loss: 0.14559842"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 800/90752, Loss: 0.10167288"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 90752/90752, Loss: 0.057396"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE: 0.9235\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming df_train and df_val are already defined\n",
    "le_user = LabelEncoder()\n",
    "le_movie = LabelEncoder()\n",
    "\n",
    "# Combine userId and movieId columns from both training and validation datasets\n",
    "combined_user_ids = pd.concat([df_train['userId'], df_val['userId']])\n",
    "combined_movie_ids = pd.concat([df_train['movieId'], df_val['movieId']])\n",
    "\n",
    "# Fit the LabelEncoder on the combined data\n",
    "le_user.fit(combined_user_ids)\n",
    "le_movie.fit(combined_movie_ids)\n",
    "\n",
    "# Transform the userId and movieId columns in both datasets\n",
    "df_train['userId'] = le_user.transform(df_train['userId'])\n",
    "df_train['movieId'] = le_movie.transform(df_train['movieId'])\n",
    "df_val['userId'] = le_user.transform(df_val['userId'])\n",
    "df_val['movieId'] = le_movie.transform(df_val['movieId'])\n",
    "\n",
    "train_dataset = MovieLensDataset(\n",
    "    users=df_train['userId'].values,\n",
    "    movies=df_train['movieId'].values,\n",
    "    ratings=df_train['rating'].values\n",
    ")\n",
    "\n",
    "val_dataset = MovieLensDataset(\n",
    "    users=df_val['userId'].values,\n",
    "    movies=df_val['movieId'].values,\n",
    "    ratings=df_val['rating'].values\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "recommendation_model = RecommendationSystemModel(\n",
    "    num_users=len(le_user.classes_), \n",
    "    num_movies=len(le_movie.classes_),\n",
    "    embedding_size=128,\n",
    "    hidden_dim=256,\n",
    "    dropout_rate=0.1,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(recommendation_model.parameters(), lr=1e-3)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# Function to log progress\n",
    "def log_progress(epoch, step, total_loss, log_progress_step, data_size, losses):\n",
    "    avg_loss = total_loss / log_progress_step\n",
    "    sys.stderr.write(f\"\\rEpoch {epoch}, Step {step}/{data_size}, Loss: {avg_loss:.6f}\")\n",
    "    sys.stderr.flush()\n",
    "\n",
    "total_loss = 0\n",
    "log_progress_step = 100\n",
    "losses = []\n",
    "train_dataset_size = len(train_dataset)\n",
    "print(f\"Training on {train_dataset_size} samples...\")\n",
    "\n",
    "recommendation_model.train()\n",
    "for e in range(EPOCHS):\n",
    "    step_count = 0  # Reset step count at the beginning of each epoch\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        output = recommendation_model(\n",
    "            train_data[\"users\"].to(device), train_data[\"movies\"].to(device)\n",
    "        )\n",
    "        # Reshape the model output to match the target's shape\n",
    "        output = output.squeeze()  # Removes the singleton dimension\n",
    "        ratings = (\n",
    "            train_data[\"ratings\"].to(torch.float32).to(device)\n",
    "        )  # Assuming ratings is already 1D\n",
    "\n",
    "        loss = loss_func(output, ratings)\n",
    "        total_loss += loss.sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Increment step count by the actual size of the batch\n",
    "        step_count += len(train_data[\"users\"])\n",
    "\n",
    "        # Check if it's time to log progress\n",
    "        if (\n",
    "            step_count % log_progress_step == 0 or i == len(train_loader) - 1\n",
    "        ):  # Log at the end of each epoch\n",
    "            log_progress(\n",
    "                e, step_count, total_loss, log_progress_step, train_dataset_size, losses\n",
    "            )\n",
    "            total_loss = 0\n",
    "    print()  # Move to the next line after each epoch\n",
    "\n",
    "# Evaluation\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "recommendation_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, valid_data in enumerate(val_loader):\n",
    "        output = recommendation_model(\n",
    "            valid_data[\"users\"].to(device), valid_data[\"movies\"].to(device)\n",
    "        )\n",
    "        ratings = valid_data[\"ratings\"].to(device)\n",
    "        y_pred.extend(output.cpu().numpy())\n",
    "        y_true.extend(ratings.cpu().numpy())\n",
    "\n",
    "# Calculate RMSE\n",
    "rms = root_mean_squared_error(y_true, y_pred)\n",
    "print(f\"RMSE: {rms:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Precision@10: 0.0008\n",
      "Recall@10: 0.0007\n",
      "NDCG@10: 0.0010\n",
      "\n",
      "Getting recommendations for user 155:\n",
      "\n",
      "Top 10 Recommended Movies:\n",
      "--------------------------------------------------------------------------------\n",
      "1. All the Vermeers in New York (1990) (Comedy|Drama|Romance)\n",
      "2. Come and See (Idi i smotri) (1985) (Drama|War)\n",
      "3. Jetée, La (1962) (Romance|Sci-Fi)\n",
      "4. Bad Boy Bubby (1993) (Drama)\n",
      "5. Summer's Tale, A (Conte d'été) (1996) (Comedy|Drama|Romance)\n",
      "6. Army of Shadows (L'armée des ombres) (1969) (Action|Drama|Thriller|War)\n",
      "7. Tom and Jerry: A Nutcracker Tale (2007) (Animation|Comedy)\n",
      "8. Big Top Scooby-Doo! (2012) (Animation|Children|Comedy)\n",
      "9. Captain Fantastic (2016) (Drama)\n",
      "10. Cosmos ((no genres listed))\n"
     ]
    }
   ],
   "source": [
    "def get_unwatched_movies(user_id, df_train):\n",
    "    user_rated_movies = set(df_train[df_train['userId'] == user_id]['movieId'].values)\n",
    "    all_movies = set(df_train['movieId'].unique())\n",
    "    return list(all_movies - user_rated_movies)\n",
    "\n",
    "def get_top_n_recommendations(model, user_id, n=10, df_train=df_train):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get unwatched movies\n",
    "        unwatched_movies = get_unwatched_movies(user_id, df_train)\n",
    "        if len(unwatched_movies) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Convert to tensor format\n",
    "        user_tensor = torch.tensor([user_id] * len(unwatched_movies)).to(device)\n",
    "        movie_tensor = torch.tensor(unwatched_movies).to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model(user_tensor, movie_tensor)\n",
    "        predictions = predictions.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Get top N items\n",
    "        top_n_indices = np.argsort(predictions)[-n:][::-1]\n",
    "        recommended_movies = [unwatched_movies[i] for i in top_n_indices]\n",
    "        \n",
    "        return recommended_movies\n",
    "\n",
    "def calculate_precision_recall(recommendations, test_items):\n",
    "    if len(recommendations) == 0 or len(test_items) == 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    true_positives = len(set(recommendations) & set(test_items))\n",
    "    precision = true_positives / len(recommendations)\n",
    "    recall = true_positives / len(test_items)\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "def calculate_ndcg(recommendations, test_items, k=10):\n",
    "    recommendations = np.array(recommendations)\n",
    "    test_items = np.array(test_items)\n",
    "    if recommendations.size == 0 or test_items.size == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    relevance = np.zeros(k)\n",
    "    for i, item in enumerate(recommendations[:k]):\n",
    "        if item in test_items:\n",
    "            relevance[i] = 1\n",
    "            \n",
    "    # Calculate DCG\n",
    "    dcg = np.sum(relevance / np.log2(np.arange(2, len(relevance) + 2)))\n",
    "    \n",
    "    # Calculate IDCG\n",
    "    ideal_relevance = np.zeros(k)\n",
    "    ideal_relevance[:min(k, len(test_items))] = 1\n",
    "    idcg = np.sum(ideal_relevance / np.log2(np.arange(2, len(ideal_relevance) + 2)))\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# Example usage\n",
    "def evaluate_recommendations(model, df_train, df_val, n=10):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    unique_users = df_val['userId'].unique()\n",
    "    \n",
    "    for user_id in unique_users:\n",
    "        # Get recommendations\n",
    "        recommendations = get_top_n_recommendations(model, user_id, n, df_train)\n",
    "        \n",
    "        # Get ground truth from validation set\n",
    "        test_items = df_val[df_val['userId'] == user_id]['movieId'].values\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision, recall = calculate_precision_recall(recommendations, test_items)\n",
    "        ndcg = calculate_ndcg(recommendations, test_items, k=n)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "    \n",
    "    return {\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls),\n",
    "        'ndcg': np.mean(ndcgs)\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "metrics = evaluate_recommendations(recommendation_model, df_train, df_val)\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"Precision@10: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall@10: {metrics['recall']:.4f}\")\n",
    "print(f\"NDCG@10: {metrics['ndcg']:.4f}\")\n",
    "\n",
    "# Load movies dataset\n",
    "movies_df = pd.read_csv(\"databases/ml-latest-small/movies.csv\")\n",
    "\n",
    "def get_movie_titles(movie_ids, movies_df=movies_df):\n",
    "    # Convert encoded IDs back to original movie IDs\n",
    "    original_movie_ids = le_movie.inverse_transform(movie_ids)\n",
    "    # Get movie titles\n",
    "    movie_info = movies_df[movies_df['movieId'].isin(original_movie_ids)]\n",
    "    return movie_info[['movieId', 'title', 'genres']].values.tolist()\n",
    "\n",
    "# Example: Get recommendations for a sample user\n",
    "sample_user_id = df_train['userId'].iloc[0]  # Get first user as example\n",
    "print(f\"\\nGetting recommendations for user {sample_user_id}:\")\n",
    "\n",
    "# Get top 10 recommendations\n",
    "recommended_movie_ids = get_top_n_recommendations(recommendation_model, sample_user_id, n=10)\n",
    "recommended_movies = get_movie_titles(recommended_movie_ids)\n",
    "\n",
    "print(\"\\nTop 10 Recommended Movies:\")\n",
    "print(\"-\" * 80)\n",
    "for i, (movie_id, title, genres) in enumerate(recommended_movies, 1):\n",
    "    print(f\"{i}. {title} ({genres})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
